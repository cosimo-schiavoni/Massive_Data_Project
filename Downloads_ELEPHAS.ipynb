{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cosimo-schiavoni/Massive_Data_Project/blob/main/Downloads_ELEPHAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8VThIbCShB9L",
        "outputId": "067b0aaa-e293-403a-dfd7-04c17aea4798",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "8VThIbCShB9L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat May 14 14:17:43 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www-eu.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!rm spark-3.2.1-bin-hadoop2.7.tgz\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install findspark\n",
        "!pip install pyspark\n",
        "!pip install -q kaggle\n",
        "!pip install elephas\n",
        "!pip install sparkdl\n",
        "!pip install tensorframes\n",
        "!pip install petastorm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcEQwefubdsC",
        "outputId": "44814c41-8a44-469f-da99-2a1e092ef0f7"
      },
      "id": "hcEQwefubdsC",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 32 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 49.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=a2ccca542965283fd6f89e38838d25cbf94f0a89ac3f56c78372da0f5fef24b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n",
            "Collecting elephas\n",
            "  Downloading elephas-3.1.0.tar.gz (26 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from elephas) (0.29.28)\n",
            "Requirement already satisfied: tensorflow!=2.2.*,>=2 in /usr/local/lib/python3.7/dist-packages (from elephas) (2.8.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from elephas) (1.1.4)\n",
            "Collecting h5py==3.3.0\n",
            "  Downloading h5py-3.3.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 31.6 MB/s \n",
            "\u001b[?25hCollecting pyspark==3.2\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 40 kB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py==3.3.0->elephas) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py==3.3.0->elephas) (1.21.6)\n",
            "Collecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 40.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (0.5.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (0.25.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (14.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (4.2.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 61.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.14.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow!=2.2.*,>=2->elephas) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (3.3.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow!=2.2.*,>=2->elephas) (3.2.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->elephas) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->elephas) (2.11.3)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->elephas) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->elephas) (2.0.1)\n",
            "Building wheels for collected packages: elephas, pyspark\n",
            "  Building wheel for elephas (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elephas: filename=elephas-3.1.0-py3-none-any.whl size=26259 sha256=bc88b2615f1c3ca3f6032212b2a9eb5700838c024485c2d51461e78ea6ec06a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/e4/e1/56dda8be927bb0e9971cd7ddf3fc1b17ce78db56268b1f867f\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805911 sha256=5399f6dd96db1107aa9a58fa1319290e7d02320a3ef90a4bf67793b5bc9d93e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built elephas pyspark\n",
            "Installing collected packages: tf-estimator-nightly, py4j, h5py, pyspark, elephas\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.3\n",
            "    Uninstalling py4j-0.10.9.3:\n",
            "      Successfully uninstalled py4j-0.10.9.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.2.1\n",
            "    Uninstalling pyspark-3.2.1:\n",
            "      Successfully uninstalled pyspark-3.2.1\n",
            "Successfully installed elephas-3.1.0 h5py-3.3.0 py4j-0.10.9.2 pyspark-3.2.0 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Collecting sparkdl\n",
            "  Downloading sparkdl-0.2.2-py3-none-any.whl (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: sparkdl\n",
            "Successfully installed sparkdl-0.2.2\n",
            "Collecting tensorframes\n",
            "  Downloading tensorframes-0.2.9-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: tensorframes\n",
            "Successfully installed tensorframes-0.2.9\n",
            "Collecting petastorm\n",
            "  Downloading petastorm-0.11.4-py2.py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 25.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from petastorm) (0.16.0)\n",
            "Requirement already satisfied: psutil>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from petastorm) (5.4.8)\n",
            "Requirement already satisfied: pyspark>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from petastorm) (3.2.0)\n",
            "Requirement already satisfied: packaging>=15.0 in /usr/local/lib/python3.7/dist-packages (from petastorm) (21.3)\n",
            "Collecting diskcache>=3.0.0\n",
            "  Downloading diskcache-5.4.0-py3-none-any.whl (44 kB)\n",
            "\u001b[K     |████████████████████████████████| 44 kB 3.2 MB/s \n",
            "\u001b[?25hCollecting fsspec\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from petastorm) (6.0.1)\n",
            "Requirement already satisfied: pyzmq>=14.0.0 in /usr/local/lib/python3.7/dist-packages (from petastorm) (22.3.0)\n",
            "Requirement already satisfied: pandas>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from petastorm) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from petastorm) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from petastorm) (1.15.0)\n",
            "Requirement already satisfied: dill>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from petastorm) (0.3.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=15.0->petastorm) (3.0.8)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.0->petastorm) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.19.0->petastorm) (2.8.2)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark>=2.1.0->petastorm) (0.10.9.2)\n",
            "Installing collected packages: fsspec, diskcache, petastorm\n",
            "Successfully installed diskcache-5.4.0 fsspec-2022.3.0 petastorm-0.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "# Spark Session, Pipeline, Functions, and Metrics\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import rand\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "\n",
        "# Keras / Deep Learning\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras import optimizers, regularizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Elephas for Deep Learning on Spark\n",
        "from elephas.ml_model import ElephasEstimator"
      ],
      "metadata": {
        "id": "tqnJaakBOJ9k"
      },
      "id": "tqnJaakBOJ9k",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.image import ImageSchema\n",
        "from pyspark.sql.functions import lit\n",
        "from functools import reduce\n",
        "from google.colab import files\n",
        "import os\n",
        "import zipfile\n",
        "import findspark\n",
        "from pyspark.sql.functions import col\n",
        "from petastorm import TransformSpec\n",
        "from petastorm.spark import SparkDatasetConverter, make_spark_converter\n",
        "import numpy as np\n",
        "\n",
        "#from my model\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "import pickle\n",
        "import shutil\n",
        "import random\n",
        "import skimage.io as io\n",
        "from copy import deepcopy\n",
        "\n"
      ],
      "metadata": {
        "id": "uI6gi1ZpNmIA",
        "outputId": "48939d59-2e68-4f3a-811f-b4ee552216b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uI6gi1ZpNmIA",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/petastorm/spark/spark_dataset_converter.py:28: FutureWarning: pyarrow.LocalFileSystem is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
            "  from pyarrow import LocalFileSystem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "import kaggle\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "AuoAPGS5afc7",
        "outputId": "f54106b6-884f-42ba-e81d-006a20f85805"
      },
      "id": "AuoAPGS5afc7",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1edf9210-e550-4735-bc4e-69d76556899f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1edf9210-e550-4735-bc4e-69d76556899f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"data_source\"] = \"./Face_Comics_data\"\n",
        "\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "if not os.path.exists(os.environ[\"data_source\"]):\n",
        "  os.makedirs(os.environ[\"data_source\"])\n",
        "  if \"comic-faces-paired-synthetic-v2\" not in os.listdir():\n",
        "    if \"comic-faces-paired-synthetic-v2.zip\" not in os.listdir():\n",
        "      ! kaggle datasets download -d defileroff/comic-faces-paired-synthetic-v2\n",
        "      with zipfile.ZipFile(\"comic-faces-paired-synthetic-v2.zip\", 'r') as f:\n",
        "        f.extractall(\"comic-faces-paired-synthetic-v2\")\n",
        "    os.remove(\"comic-faces-paired-synthetic-v2.zip\")\n",
        "\n",
        "\n",
        "!mv \"./comic-faces-paired-synthetic-v2/face2comics_v2.0.0_by_Sxela/face2comics_v2.0.0_by_Sxela/comics\" \"./Face_Comics_data\"\n",
        "!mv \"./comic-faces-paired-synthetic-v2/face2comics_v2.0.0_by_Sxela/face2comics_v2.0.0_by_Sxela/faces\" \"./Face_Comics_data\"\n",
        "\n",
        "!rm -rf comic-faces-paired-synthetic-v2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A8XXBVUU_o_",
        "outputId": "c8bf4413-eb16-404f-d32a-fe46c09d96e4"
      },
      "id": "4A8XXBVUU_o_",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading comic-faces-paired-synthetic-v2.zip to /content\n",
            " 99% 2.15G/2.18G [00:18<00:00, 142MB/s]\n",
            "100% 2.18G/2.18G [00:18<00:00, 128MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\"\n",
        "\n",
        "findspark.init(findspark.find())\n",
        "#findspark.init(findspark.find())\n",
        "#spark = SparkSession.builder.master(\"local\").appName(\"Colab\").getOrCreate()\n",
        "#spark\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"Colab\").getOrCreate()\n",
        "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"Colab\")\n",
        "conf.set(\"spark.executor.memory\", \"6G\")\n",
        "conf.set(\"spark.driver.memory\", \"2G\")\n",
        "conf.set(\"spark.executor.cores\", \"4\")\n",
        "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
        "conf.set(\"spark.default.parallelism\", \"4\")\n",
        "sc =  SparkContext.getOrCreate(conf=conf)\n",
        "spark"
      ],
      "metadata": {
        "id": "4SO-9l2tqKkO",
        "outputId": "e8cb7d82-5f41-4b07-fb5e-7cd71331bd24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "id": "4SO-9l2tqKkO",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f843f55fdd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0d240f1bfe52:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sql_context = SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)"
      ],
      "metadata": {
        "id": "zqogXr85xr5J"
      },
      "id": "zqogXr85xr5J",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_comics = spark.read.format(\"binaryFile\").load(\".//Face_Comics_data//comics\").withColumn(\"label\", lit(int(1)))\n",
        "df_faces = spark.read.format(\"binaryFile\").load(\".//Face_Comics_data//faces\").withColumn(\"label\", lit(int(0)))\n",
        "\n",
        "#df_comics.select(\"image.height\",\"image.width\",\"image.nChannels\", \"image.mode\", \"image.data\").show(1)\n",
        "#df_comics.show(5)\n",
        "df_comics = df_comics.select(col(\"content\"),col(\"label\"))\n",
        "df_faces = df_faces.select(col(\"content\"),col(\"label\"))\n",
        "\n",
        "df_comics.show(5)\n",
        "df_faces.show(5)"
      ],
      "metadata": {
        "id": "EbQikNtdyHq_",
        "outputId": "d12c3b07-9a39-4cd8-9f23-a3f0d51838ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EbQikNtdyHq_",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|             content|label|\n",
            "+--------------------+-----+\n",
            "|[FF D8 FF E0 00 1...|    1|\n",
            "|[FF D8 FF E0 00 1...|    1|\n",
            "|[FF D8 FF E0 00 1...|    1|\n",
            "|[FF D8 FF E0 00 1...|    1|\n",
            "|[FF D8 FF E0 00 1...|    1|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+--------------------+-----+\n",
            "|             content|label|\n",
            "+--------------------+-----+\n",
            "|[FF D8 FF E0 00 1...|    0|\n",
            "|[FF D8 FF E0 00 1...|    0|\n",
            "|[FF D8 FF E0 00 1...|    0|\n",
            "|[FF D8 FF E0 00 1...|    0|\n",
            "|[FF D8 FF E0 00 1...|    0|\n",
            "+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*****************************************!NB Setup Single node test*****************************************"
      ],
      "metadata": {
        "id": "lP7hjkspnbTA"
      },
      "id": "lP7hjkspnbTA"
    },
    {
      "cell_type": "code",
      "source": [
        "conf = SparkConf().setAppName('Face_Comics_recognition').setMaster('local[6]')\n",
        "sc = SparkContext(conf=conf)\n",
        "sql_context = SQLContext(sc)"
      ],
      "metadata": {
        "id": "W6ZzRSUUN9HD",
        "outputId": "90517434-0976-47dc-8989-540d808bf594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "W6ZzRSUUN9HD",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop2.7\"\n",
        "\n",
        "\n",
        "findspark.init(findspark.find())\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"Colab\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "3W8In5XxmceV",
        "outputId": "bc397f94-6fba-4111-93d3-01018b9d2119"
      },
      "id": "3W8In5XxmceV",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff588d42410>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0d240f1bfe52:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[6]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Face_Comics_recognition</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#limit files in order to test the algorithm before to distribute and scale\n",
        "\n",
        "#os.makedirs(\"./a/sample_data\")\n",
        "#! ls -lt sample_data/*.csv | head -3 | awk '{print \"mv \" $9 \" a/\"$9}' | sh\n",
        "#! rm -rf './sample_data'\n",
        "#! mv './a/sample_data' './'\n",
        "#! rm -rf './a'\n",
        "\n",
        "os.makedirs(\"./a/Face_Comics_data/comics\")\n",
        "os.makedirs(\"./a/Face_Comics_data/faces\")\n",
        "! ls -lt Face_Comics_data/comics/*.jpg | head -10 | awk '{print \"mv \" $9 \" a/\"$9}' | sh\n",
        "! ls -lt Face_Comics_data/faces/*.jpg | head -10 | awk '{print \"mv \" $9 \" a/\"$9}' | sh\n",
        "! rm -rf './Face_Comics_data'\n",
        "! mv './a/Face_Comics_data/' './'\n",
        "! rm -rf './a'\n"
      ],
      "metadata": {
        "id": "YHVxRjAnzH1_"
      },
      "id": "YHVxRjAnzH1_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_comics = spark.read.format(\"binaryFile\").load(\".//Face_Comics_data//comics\").withColumn(\"label\", lit(int(1)))\n",
        "df_faces = spark.read.format(\"binaryFile\").load(\".//Face_Comics_data//faces\").withColumn(\"label\", lit(int(0)))\n",
        "\n",
        "#df_comics.select(\"image.height\",\"image.width\",\"image.nChannels\", \"image.mode\", \"image.data\").show(1)\n",
        "#df_comics.show(5)\n",
        "df_comics = df_comics.select(col(\"content\"),col(\"label\"))\n",
        "df_faces = df_faces.select(col(\"content\"),col(\"label\"))\n",
        "\n",
        "#df_comics.show(5)\n",
        "#df_faces.show(5)"
      ],
      "metadata": {
        "id": "k-XWtnnqEpAO"
      },
      "id": "k-XWtnnqEpAO",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_comics = spark.read.format(\"image\").load(\".//Face_Comics_data//comics\").withColumn(\"label\", lit(int(1)))\n",
        "df_faces = spark.read.format(\"image\").load(\".//Face_Comics_data//faces\").withColumn(\"label\", lit(int(0)))\n",
        "\n",
        "#df_comics.select(\"image.height\",\"image.width\",\"image.nChannels\", \"image.mode\", \"image.data\").show(1)\n",
        "#df_comics.show(5)\n",
        "df_comics = df_comics.select(col(\"image.data\"),col(\"label\"))\n",
        "df_faces = df_faces.select(col(\"image.data\"),col(\"label\"))\n",
        "\n",
        "#df_comics.show(5)\n",
        "#df_faces.show(5)"
      ],
      "metadata": {
        "id": "683i2ckq0X-E"
      },
      "id": "683i2ckq0X-E",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read images and Create training & test DataFrames for transfer learning\n",
        "#Shuffle Data\n",
        "df_comics = df_comics.orderBy(rand())\n",
        "df_faces = df_faces.orderBy(rand())\n",
        "\n",
        "comics_train, comics_validation, comics_test = df_comics.randomSplit([0.6, 0.2,0.2], seed = 1234)\n",
        "print(comics_train)\n",
        "faces_train, faces_validation, faces_test = df_faces.randomSplit([0.6, 0.2,0.2], seed = 1234)\n",
        "print(faces_train)\n",
        "\n",
        "#dataframe for training a classification model\n",
        "train_df = faces_train.unionAll(comics_train)\n",
        "\n",
        "#dataframe for validating the classification model\n",
        "val_df = faces_validation.unionAll(comics_validation)\n",
        "\n",
        "#dataframe for testing the classification model\n",
        "test_df = faces_test.unionAll(comics_test)\n",
        "\n",
        "#num_classes = test_df.select(\"label\").distinct().count()\n",
        "train_df = train_df.repartition(2)\n",
        "#print(\"number of classes\", num_classes)"
      ],
      "metadata": {
        "id": "9PwrfcDZH7BO",
        "outputId": "3f7f3f5c-1773-417f-bd33-fc5a673dc90f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9PwrfcDZH7BO",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame[content: binary, label: int]\n",
            "DataFrame[content: binary, label: int]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "id": "Z7WgsPB3yADx",
        "outputId": "fda0e30e-1416-4c4f-ab28-a1e96016179f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Z7WgsPB3yADx",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[data: binary, label: int]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf './staging'"
      ],
      "metadata": {
        "id": "RmRDam-t5TZV"
      },
      "id": "RmRDam-t5TZV",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def preprocess(content):\n",
        "  image = Image.open(io.BytesIO(content)).resize([224,224])\n",
        "  image_array = keras.preprocessing.image.img_to_array(image)\n",
        "  #return tf.keras.applications.mobilnte_v2.preprocess_input(image_array)\n",
        "  return image_array\n",
        "\n",
        "def transform_batch(pd_batch):\n",
        "  pd_batch['features'] = pd_batch['content'].map(lambda x: preprocess(x))\n",
        "  pd_batch = pd_batch.drop(labels='content', axis = 1)\n",
        "  return pd_batch\n",
        "\n",
        "IMG_SHAPE = (224, 224, 3)\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (224, 224, 3)\n",
        "\n",
        "transform_spec_fn = TransformSpec(\n",
        "    transform_batch,\n",
        "    edit_fields = [('features', np.float32, IMG_SHAPE, False)],\n",
        "    selected_fields = ['features','label']\n",
        ")"
      ],
      "metadata": {
        "id": "QVyyE5t0zZDn"
      },
      "id": "QVyyE5t0zZDn",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd_batch0 = train_df.limit(5).toPandas()\n",
        "pd_batch0"
      ],
      "metadata": {
        "id": "65nqGp2v83XR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "4bcedb30-7d7e-41f1-c05c-425931c006dc"
      },
      "id": "65nqGp2v83XR",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             content  label\n",
              "0  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...      0\n",
              "1  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...      0\n",
              "2  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...      0\n",
              "3  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...      0\n",
              "4  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a49f185c-e5a9-4e87-a54b-4b8c3c9559d3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a49f185c-e5a9-4e87-a54b-4b8c3c9559d3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a49f185c-e5a9-4e87-a54b-4b8c3c9559d3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a49f185c-e5a9-4e87-a54b-4b8c3c9559d3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, \"file:/content/staging\")\n",
        "\n",
        "converter_train = make_spark_converter(train_df)\n",
        "converter_val = make_spark_converter(val_df)"
      ],
      "metadata": {
        "id": "lFH6KMdLGWLy",
        "outputId": "bca5a8a0-f3d8-46ff-afd9-21453527dd6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "lFH6KMdLGWLy",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
            "  self._filesystem = pyarrow.localfs\n",
            "Converting floating-point columns to float32\n",
            "Converting floating-point columns to float32\n",
            "The median size 5673339 B (< 50 MB) of the parquet files is too small. Total size: 463040157 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///content/staging/20220515225541-appid-local-1652655118499-4ab63dbf-c0db-4b16-b9ad-e3897757e120/part-00057-5dd4226a-5a71-4dc8-b8c4-03e7cc57e278-c000.parquet, ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"#pesca dalla folder scaricata, non dal flusso spark\n",
        "data_dir= os.environ[\"data_source\"]\n",
        "\n",
        "os.listdir(data_dir)\n",
        "\n",
        "tf.random.set_seed(123456)\n",
        "\n",
        "\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.3,\n",
        "    subset=\"training\",\n",
        "    shuffle=True,\n",
        "    seed=123456,\n",
        "    image_size= (160,160),\n",
        "    batch_size=32)\n",
        "\n",
        "\n",
        "class_names = train_data.select(\"label_index\").distinct().count()\n",
        "print(class_names)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "x2eIzIjqpSPs"
      },
      "id": "x2eIzIjqpSPs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with  converter_train.make_tf_dataset(transform_spec=transform_spec_fn, \n",
        "                                  batch_size = BATCH_SIZE) as train_dataset,\\\n",
        "        converter_val.make_tf_dataset(transform_spec=transform_spec_fn, \n",
        "                                  batch_size = BATCH_SIZE) as val_dataset:\n",
        "                                    \n",
        "    train_dataset_ = train_dataset.map(lambda x: (x.features, x.label))\n",
        "    validation_dataset_ = val_dataset.map(lambda x: (x.features, x.label))"
      ],
      "metadata": {
        "id": "qFu_uII7iT7Z",
        "outputId": "a9e477ce-73cb-4da0-b238-3b36f35b38c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qFu_uII7iT7Z",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
            "  self._filesystem = pyarrow.localfs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir= \"./Face_Comics_data\"\n",
        "os.listdir(data_dir)\n",
        "\n",
        "\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.3,\n",
        "    subset=\"training\",\n",
        "    shuffle=True,\n",
        "    seed=123456,\n",
        "    image_size= IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.3,\n",
        "    subset=\"validation\",\n",
        "    shuffle=True,\n",
        "    seed=123456,\n",
        "    image_size= IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "4rL2iR7iubBq",
        "outputId": "91b75963-65af-4407-9765-e87a7e1cd3ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4rL2iR7iubBq",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Using 14000 files for training.\n",
            "Found 20000 files belonging to 2 classes.\n",
            "Using 6000 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset"
      ],
      "metadata": {
        "id": "ENKThWIcuyBc",
        "outputId": "28588564-2ffb-4c23-a720-bff84036591d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ENKThWIcuyBc",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset_"
      ],
      "metadata": {
        "id": "h0Uqv-r0vPEO",
        "outputId": "d44c5d82-5bd2-44eb-a2d1-5334bb3e869c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "h0Uqv-r0vPEO",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wEiXnqLBv37l"
      },
      "id": "wEiXnqLBv37l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = tf.keras.models.Sequential()\n",
        "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size=3, strides=2, activation='relu',  input_shape = [224,224,3]))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))\n",
        "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3,  strides=2, activation = 'relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))\n",
        "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 2,  strides=2, activation = 'relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides=1, padding='same'))\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))\n",
        "cnn.add(tf.keras.layers.Dropout(0.1))\n",
        "cnn.add(tf.keras.layers.Dense(units=1, activation = 'sigmoid'))\n",
        "\n",
        "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "model_fit = cnn.fit(x = train_dataset_, validation_data = validation_dataset_, epochs = EPOCHS)\n",
        "\n",
        "cnn.summary()"
      ],
      "metadata": {
        "id": "V9bBzlX9Ie05",
        "outputId": "e4d3d1c6-ed41-4158-9f45-c61986d46f65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "V9bBzlX9Ie05",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-901d337416a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel_fit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_dataset_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nRuntimeError: Trying to read a sample after a reader created by make_reader/make_batch_reader has stopped. This may happen if the make_reader/make_batch_reader context manager has exited but you try to fetch a sample from it anyway\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.7/dist-packages/petastorm/tf_utils.py\", line 380, in dequeue_sample_impl\n    for row in reader:\n\n  File \"/usr/local/lib/python3.7/dist-packages/petastorm/reader.py\", line 667, in __next__\n    raise RuntimeError('Trying to read a sample after a reader created by '\n\nRuntimeError: Trying to read a sample after a reader created by make_reader/make_batch_reader has stopped. This may happen if the make_reader/make_batch_reader context manager has exited but you try to fetch a sample from it anyway\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_7554]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(SparkDatasetConverter.PARENT_CACHE_DIR_URL_CONF, \"file:/content/staging\")\n",
        "\n",
        "converter_train = make_spark_converter(train_df)\n",
        "converter_val = make_spark_converter(val_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GrqypoBJ2Gu",
        "outputId": "bc3583d3-bb41-43e3-b06d-b5938d20eb9f"
      },
      "id": "-GrqypoBJ2Gu",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/petastorm/fs_utils.py:88: FutureWarning: pyarrow.localfs is deprecated as of 2.0.0, please use pyarrow.fs.LocalFileSystem instead.\n",
            "  self._filesystem = pyarrow.localfs\n",
            "The median size 5644755 B (< 50 MB) of the parquet files is too small. Total size: 462418027 B. Increase the median file size by calling df.repartition(n) or df.coalesce(n), which might help improve the performance. Parquet files: file:///content/staging/20220515191308-appid-local-1652641712749-e4920d43-7e43-472e-836f-e10763ca4330/part-00069-c04df76c-2fb0-413f-8bd9-b66786401763-c000.parquet, ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter_val"
      ],
      "metadata": {
        "id": "H4I7kmwGu3Lc",
        "outputId": "4ab9ca7c-67ae-4191-e00f-c137eefc47c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "H4I7kmwGu3Lc",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<petastorm.spark.spark_dataset_converter.SparkDatasetConverter at 0x7ff58280b790>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from elephas.utils.rdd_utils import to_simple_rdd\n",
        "from elephas.spark_model import SparkModel\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf \n",
        "\n",
        "optimizer_conf = tf.keras.optimizers.Adam(lr=0.01)\n",
        "opt_conf = optimizers.serialize(optimizer_conf)\n",
        "\n",
        "# Initialize Elephas Spark ML Estimator\n",
        "estimator = ElephasEstimator()\n",
        "estimator.setFeaturesCol(\"content\")\n",
        "estimator.setLabelCol(\"label\")\n",
        "estimator.set_keras_model_config(cnn.to_json())\n",
        "estimator.set_categorical_labels(True)\n",
        "estimator.set_nb_classes(num_classes)\n",
        "estimator.set_num_workers(1)\n",
        "estimator.set_epochs(15)\n",
        "estimator.set_batch_size(32)\n",
        "estimator.set_verbosity(1)\n",
        "estimator.set_validation_split(0.20)\n",
        "estimator.set_optimizer_config(opt_conf)\n",
        "estimator.set_mode(\"synchronous\")\n",
        "estimator.set_loss(\"categorical_crossentropy\")\n",
        "estimator.set_metrics(['acc'])\n",
        "\n",
        "\n",
        "# Fitting a model returns a Transformer\n",
        "pipeline = Pipeline(stages=[estimator])\n",
        "\n",
        "\"\"\"\n",
        "fitted_pipeline = pipeline.fit(train_df)\n",
        "\n",
        "# Evaluate Spark model\n",
        "prediction = fitted_pipeline.transform(train_df)\n",
        "pnl = prediction.select(\"index_category\", \"prediction\")\n",
        "pnl.show\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "fit_dl_pipeline = pipeline.fit(train_data)\n",
        "pred_train = fit_dl_pipeline.transform(train_data)\n",
        "pred_test = fit_dl_pipeline.transform(test_data)\n",
        "\n",
        "pnl_train = pred_train.select(label, \"prediction\")\n",
        "pnl_test = pred_test.select(label, \"prediction\")\n",
        "\n",
        "pred_and_label_train = pnl_train.rdd.map(lambda row: (row[label], row['prediction']))\n",
        "pred_and_label_test = pnl_test.rdd.map(lambda row: (row[label], row['prediction']))\n",
        "\n",
        "metrics_train = MulticlassMetrics(pred_and_label_train)\n",
        "metrics_test = MulticlassMetrics(pred_and_label_tes\n",
        "\n",
        "print(\"Training data accuracy: {}\".format(round(metrics_train.precision(),4)))\n",
        "print(\"Training Data Confusion Matrix\")\n",
        "display(pnl_train.crosstab('label','prediction').toPandas())\n",
        "\n",
        "print(\"\\nTest data accuracy: {}\".format(round(metrics_test.precision(),4)))\n",
        "print(\"Test Data Confusion Matrix\")\n",
        "display(pnl_test.crosstab('label','prediction').toPandas())\n",
        "\"\"\"\n",
        "\n",
        "#hist = cnn.fit(train_dataset, validation_data = val_dataset, epochs = EPOCHS)\n",
        "#return hist.history['val_loss'][-1], hist.history['val_accuracy'][-1]\n",
        "#return \n",
        "\n",
        "#loss, accuracy = training_and_validation()\n",
        "#print('Validation accuracy: {}'.format(accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "Hbz1-cbmKEIi",
        "outputId": "901594f6-e1d1-4432-d0b0-6e85eaf1b459"
      },
      "id": "Hbz1-cbmKEIi",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfit_dl_pipeline = pipeline.fit(train_data)\\npred_train = fit_dl_pipeline.transform(train_data)\\npred_test = fit_dl_pipeline.transform(test_data)\\n\\npnl_train = pred_train.select(label, \"prediction\")\\npnl_test = pred_test.select(label, \"prediction\")\\n\\npred_and_label_train = pnl_train.rdd.map(lambda row: (row[label], row[\\'prediction\\']))\\npred_and_label_test = pnl_test.rdd.map(lambda row: (row[label], row[\\'prediction\\']))\\n\\nmetrics_train = MulticlassMetrics(pred_and_label_train)\\nmetrics_test = MulticlassMetrics(pred_and_label_tes\\n\\nprint(\"Training data accuracy: {}\".format(round(metrics_train.precision(),4)))\\nprint(\"Training Data Confusion Matrix\")\\ndisplay(pnl_train.crosstab(\\'label\\',\\'prediction\\').toPandas())\\n\\nprint(\"\\nTest data accuracy: {}\".format(round(metrics_test.precision(),4)))\\nprint(\"Test Data Confusion Matrix\")\\ndisplay(pnl_test.crosstab(\\'label\\',\\'prediction\\').toPandas())\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dl_pipeline_fit_score_results(dl_pipeline=pipeline,\n",
        "                                  train_data = train_dataset,\n",
        "                                  test_data = val_dataset,\n",
        "                                  label = 'label'):\n",
        "  \n",
        "    fit_dl_pipeline = dl_pipeline.fit(train_data)\n",
        "    pred_train = fit_dl_pipeline.transform(train_data)\n",
        "    pred_test = fit_dl_pipeline.transform(test_data)\n",
        "\n",
        "    pnl_train = pred_train.select(label, \"prediction\")\n",
        "    pnl_test = pred_test.select(label, \"prediction\")\n",
        "\n",
        "    pred_and_label_train = pnl_train.rdd.map(lambda row: (row[label], row['prediction']))\n",
        "    pred_and_label_test = pnl_test.rdd.map(lambda row: (row[label], row['prediction']))\n",
        "\n",
        "    metrics_train = MulticlassMetrics(pred_and_label_train)\n",
        "    metrics_test = MulticlassMetrics(pred_and_label_test)\n",
        "\n",
        "    print(\"Training data accuracy: {}\".format(round(metrics_train.precision(),4)))\n",
        "    print(\"Training Data Confusion Matrix\")\n",
        "    display(pnl_train.crosstab('label','prediction').toPandas())\n",
        "\n",
        "    print(\"\\nTest data accuracy: {}\".format(round(metrics_test.precision(),4)))\n",
        "    print(\"Test Data Confusion Matrix\")\n",
        "    display(pnl_test.crosstab('label','prediction').toPandas())"
      ],
      "metadata": {
        "id": "_12CuLirQiYh"
      },
      "id": "_12CuLirQiYh",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dl_pipeline_fit_score_results(dl_pipeline=pipeline,\n",
        "                                  train_data = train_dataset,\n",
        "                                  test_data = val_dataset,\n",
        "                                  label = 'label')"
      ],
      "metadata": {
        "id": "LVx1syn2q_Yy",
        "outputId": "cd51f4f9-a99b-4b0f-b7bb-d7b9c9b5b367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "id": "LVx1syn2q_Yy",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-4b731934276e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                                   \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                   \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                   label = 'label')\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-104663934db2>\u001b[0m in \u001b[0;36mdl_pipeline_fit_score_results\u001b[0;34m(dl_pipeline, train_data, test_data, label)\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   label = 'label'):\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfit_dl_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_dl_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpred_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_dl_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/elephas/ml_model.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \"\"\"\n\u001b[1;32m     83\u001b[0m         simple_rdd = df_to_simple_rdd(df, categorical=self.get_categorical_labels(), nb_classes=self.get_nb_classes(),\n\u001b[0;32m---> 84\u001b[0;31m                                       features_col=self.getFeaturesCol(), label_col=self.getLabelCol())\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0msimple_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mkeras_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_keras_model_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_custom_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/elephas/ml/adapter.py\u001b[0m in \u001b[0;36mdf_to_simple_rdd\u001b[0;34m(df, categorical, nb_classes, features_col, label_col)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"Convert DataFrame into RDD of pairs\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0msql_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0msql_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterDataFrameAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"temp_table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     selected_df = sql_context.sql(\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DatasetV1Adapter' object has no attribute 'sql_ctx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#There are two variable objects. Divided between around 2.5 million of MobilNet parameters which \n",
        "#are frozen, and 1.2 thousend of trainable parameter in the Dense layer\n",
        "len(cnn.trainable_variables)\n",
        "\n",
        "acc = model_fit.history['accuracy']\n",
        "val_acc = model_fit.history['val_accuracy']\n",
        "loss_ = model_fit.history['loss']\n",
        "val_loss_ = model_fit.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy', color = 'gray',linestyle='dashed')\n",
        "plt.plot(val_acc, label='Validation Accuracy', color = 'black')\n",
        "plt.ylim([0.8, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss_, label='Training Loss', color = 'gray',linestyle='dashed')\n",
        "plt.plot(val_loss_, label='Validation Loss', color = 'black')\n",
        "plt.ylim([0, 1.0])\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JeUtrqxxXElP"
      },
      "id": "JeUtrqxxXElP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Copy of Downloads.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}